# Llama Stack configuration for production with OpenAI
# Start with: llama stack run llama-stack-config/run-openai.yaml --port 8321

version: '2'
image_name: starter

apis:
  - agents
  - inference
  - safety
  - tool_runtime
  - vector_io

providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}

  vector_io:
    - provider_id: faiss
      provider_type: inline::faiss
      config:
        persistence:
          namespace: vector_io::faiss
          backend: kv_default

  safety:
    - provider_id: llama-guard
      provider_type: inline::llama-guard
      config:
        excluded_categories: []

  agents:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        persistence:
          agent_state:
            namespace: agents
            backend: kv_default
          responses:
            table_name: responses
            backend: sql_default
            max_write_queue_size: 10000
            num_writers: 4

  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}

storage:
  backends:
    kv_default:
      type: kv_sqlite
      db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/openai}/kvstore.db
    sql_default:
      type: sql_sqlite
      db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/openai}/sql_store.db
  stores:
    metadata:
      namespace: registry
      backend: kv_default
    inference:
      table_name: inference_store
      backend: sql_default
      max_write_queue_size: 10000
      num_writers: 4
    conversations:
      table_name: openai_conversations
      backend: sql_default

registered_resources:
  models:
    - metadata: {}
      model_id: gpt-4o
      provider_id: openai
      model_type: llm
  shields: []
  vector_dbs: []
  datasets: []
  scoring_fns: []
  benchmarks: []
  tool_groups:
    - toolgroup_id: ecommerce-api
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://localhost:8080/mcp

server:
  port: 8321
