# Agent Provider Configuration
# Set to 'openai' to use OpenAI directly, or 'llamastack' to use Llama Stack
# Default: openai
AGENT_PROVIDER=openai
# AGENT_PROVIDER=llamastack


# OpenAI Configuration (used when AGENT_PROVIDER=openai)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini


# MCP Server Configuration (for tool calling)
# Set this to enable tool calling. Leave empty/commented to disable tools completely.
MCP_SERVER_URL=http://localhost:8080

# ============================================
# Llama Stack Configuration (OPTIONAL)
# Uncomment and set AGENT_PROVIDER=llamastack to enable
# ============================================

# Llama Stack server URL (for the Next.js frontend)
LLAMA_STACK_URL=http://localhost:8321

# Model to use with Llama Stack
# For Ollama: ollama/llama3.1:latest, ollama/llama3.2:latest, ollama/qwen3:8b, etc.
# For OpenAI via Llama Stack: openai/gpt-4o, openai/gpt-4o-mini, etc.
# Check available models with: ollama list
# INFERENCE_MODEL=ollama/llama3.1:latest
INFERENCE_MODEL=openai/gpt-4o

# Tool group ID for Llama Stack MCP integration
TOOL_GROUP_ID=ecommerce-api

# Ollama Configuration (for local development with Llama Stack)
OLLAMA_URL=http://localhost:11434

# SQLite store directory for Llama Stack
SQLITE_STORE_DIR=~/.llama/distributions/ollama
